# -*- coding: utf-8 -*-
"""Chatbot_phi2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZBxLmUAoxxUx9Tz7JW4N5DG-K2t1FTR_
"""

import json
import tensorflow as tf

# Load JSON from file or paste the JSON data directly if running in Colab
with open('thd_content.json', 'r') as f:
    data = json.load(f)

# Create a list of tuples: (title, content, url)
entries = [(item['title'], item['content'], item['url']) for item in data]

# Create a tf.data.Dataset
dataset = tf.data.Dataset.from_tensor_slices(entries) #creating a dataset from another dataset. pass it through pieces of tensors
#we created a dataset and each element of the dataset

# list(dataset.as_numpy_iterator())


for title, content, url in dataset.take(1):
    print(f"Title: {title.numpy().decode()}")
    print(f"Content: {content.numpy().decode()[:300]}...")
    print(f"URL: {url.numpy().decode()}")

!pip install -U sentence-transformers scikit-learn

from sentence_transformers import SentenceTransformer
import numpy as np

# Load the model
model = SentenceTransformer('all-MiniLM-L6-v2') #pre-trained encoder model

# Extract just the content to embed
contents = [entry[1] for entry in entries]  # index 1 = content field

# Generate embeddings (this may take a few seconds)
embeddings = model.encode(contents, show_progress_bar=True) #vectors that capture the semantic meaning of each content.

# Save embeddings to file
np.save('thd_embeddings.npy', embeddings)

# Also save titles and urls for later retrieval
with open('thd_metadata.json', 'w') as f:
    json.dump([{'title': e[0], 'url': e[2]} for e in entries], f)

print("Embeddings saved to 'thd_embeddings.npy'")
print("Metadata saved to 'thd_metadata.json'")

import numpy as np
import json
from sklearn.metrics.pairwise import cosine_similarity

# Load embeddings
embeddings = np.load('thd_embeddings.npy')

# Load metadata (titles and URLs)
with open('thd_metadata.json', 'r') as f:
    metadata = json.load(f)

def search_documents(query, k=3): #defining semantic search function
    # Embed the query using the same model
    query_embedding = model.encode([query])

    # Compute cosine similarity
    similarities = cosine_similarity(query_embedding, embeddings)[0]

    # Get top-k indices
    top_k_idx = similarities.argsort()[-k:][::-1]

    results = []
    for idx in top_k_idx:
        results.append({
            "score": float(similarities[idx]),
            "title": metadata[idx]['title'],
            "url": metadata[idx]['url'],
            "content": entries[idx][1]  # get original content
        })

    return results

# Try a test query
results = search_documents("How can I apply to DIT as an international student?", k=3)

# Display results
for i, res in enumerate(results, 1):
    print(f"\nüîπ Result {i}")
    print(f"Title: {res['title']}")
    print(f"Score: {res['score']:.4f}")
    print(f"URL: {res['url']}")
    print(f"Excerpt: {res['content'][:300]}...")

!pip install -U transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM #text generator
import torch #dl framework



# Load Phi-2
model_id = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
phi2 = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
phi2.eval().to("cuda" if torch.cuda.is_available() else "cpu")

# this section is for Phi2_ generating answer
def build_prompt(query, retrieved_docs):
    context = "\n\n".join([doc['content'][:1000] for doc in retrieved_docs])  # Keep it short and relevant
    prompt = f"""You are a helpful university assistant chatbot. Use the context below to answer the user's question clearly and concisely.

Context:
{context}

User Question:
{query}

Answer:"""
    return prompt
def generate_answer(query, top_k_docs):
    prompt = build_prompt(query, top_k_docs)

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(phi2.device)

    with torch.no_grad():
        outputs = phi2.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Trim the prompt from the output
    return answer[len(prompt):].strip()

# this is for Phi2_ TRY
query = "How do I apply for a master's degree at THD?"
top_docs = search_documents(query, k=3)

answer = generate_answer(query, top_docs)
print("ü§ñ Chatbot Answer:")
print(answer)

!pip install gradio

def chatbot_interface(user_query):
    try:
        top_docs = search_documents(user_query, k=3)
        answer = generate_answer(user_query, top_docs)

        # Optional: Add clickable sources
        sources = "\n\nSources:\n" + "\n".join(
            [f"- [{doc['title']}]({doc['url']})" for doc in top_docs]
        )

        return answer + sources

    except Exception as e:
        # Return error to the frontend
        return f"‚ö†Ô∏è An error occurred: {str(e)}"

import gradio as gr

gr.Interface(
    fn=chatbot_interface,
    inputs=gr.Textbox(lines=2, placeholder="Ask me something about THD..."),
    outputs="text",
    title="üéì THD Assistant Chatbot",
    description="Ask me anything about studying at THD! I‚Äôll search official info and respond using AI."
).launch(share=True)